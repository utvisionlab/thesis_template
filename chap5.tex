\chapter{مدل آمیخته vMF و الگوریتم پیشنهادی برای مقداردهی اولیه آن} \label{ch:vMF}
توزیع
\lr{von Mises Fisher (vMF)}
یکی از توزیع‌های رایج و پرکاربرد برای برازش به
{\mmm{داده‌های جهتی}{directional data}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{الگوریتم k-means++ برای توزیع آمیخته vMF}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec.kmeanspp}
الگوریتم
\lr{k-means++}
یک روش مقداردهی اولیه احتمالاتی به الگوریتم
\lr{k-means}
می‌افزاید که سعی می‌کند نقاطی از داده‌ها را بیابد که فاصله زیادی از یکدیگر داشته باشند و در عین حال از انتخاب نقاط
{\mmm{برون‌هشته}{outlier}}
اجتناب می‌کند.
در کار با توزیع‌های آمیخته گوسی، نقاط انتخاب شده برای مقداردهی اولیه به میانگین اجزای گوسی مدل استفاده می‌شوند.
برای توزیع‌های آمیخته vMF نیز ما مشابه همین کار را انجام می‌دهیم،
یعنی تعدادی از نقاط داده‌ها را که از یکدیگر دور باشند انتخاب نموده و از آن‌ها برای مقداردهی اولیه به پارامتر
$\vmu$
برای اجزای مختلف مدل استفاده می‌نماییم.

برای این منظور باید یک تابع فاصله مناسب برای نقاط قرار گرفته روی ابرکره واحد تعریف نماییم.
فرض کنید
$\vx$
و
$\vy$
دو نقطه روی ابرکره واحد باشند.
فاصله بین این دو نقطه را به صورت زیر تعریف می‌کنیم:
\begin{equation*}
d(\vx,\vy)= \sqrt{2- 2 \vx^T \vy}.
\end{equation*}
دلیل انتخاب این تابع فاصله در ادامه همین بخش روشن خواهد شد.
الگوریتم
\lr{k-means++}
ابتدا به صورت تصادفی یک نقطه از داده‌ها را به عنوان میانگین اولین خوشه انتخاب می‌کند.
نقطه دوم نیز به صورت تصادفی انتخاب می‌شود، اما با احتمالی متناسب با مربع فاصله از نقطه اول.
به همین ترتیب برای نقطه
$k$ام،
احتمال انتخاب هر نقطه متناسب با کمینه مربع فاصله‌های آن از 
$k-1$
نقطه انتخاب شده قبلی خواهد بود.
جزئیات کامل این روش در الگوریتم~\ref{alg.kmeanspp}
آمده است.

\begin{algorithm}[t]
	\caption{\small \lr{k-means++} برای توزیع آمیخته vMF}
	\label{alg.kmeanspp}
	\begin{algorithmic}
		\begin{latin}
		\STATE \textbf{Input:} $n$ data points on the unit hypersphere $\{\vx_i\}_{i=1}^n$; $K$, the number of components\\
		Set $\vmu_1 \gets \vx_i$, wherein $i\in\{1,...,n\}$ is chosen randomly\\
		Set $c_i \gets \vmu_1^T \vx_i$, $i=1,...,n$ 
		\FOR {$k=2$  to $K$} 
		\item Set $d_i \gets \sum_{j=1}^k (2 - 2 c_j)$, $i=1,...,n$ 
		\item Set $\vmu_k \gets \vx_i$, wherein $i\in\{1,...,n\}$ is chosen by the probability $p_i = d_i/\sum_{i=1}^n{d_i}$
		\item Set  $c_i = \max(c_i, \vmu_k^T \vx_i)$, $i=1,...,n$ 
		\ENDFOR
		\end{latin}
	\end{algorithmic}
\end{algorithm}

در حالت توزیع‌های آمیخته گوسی با تابع فاصله اقلیدسی
$d(\vx,\vy) = \norm{\vx-\vy}$،
در مرجع%
~\cite{arthur_kmeans_2007}
نشان داده شده است که الگوریتم
\lr{k-means++}
یک حد مورد انتظار جالب توجه برای تابع هدف ایجاد می‌کند، در حالی که با مقداردهی اولیه تصادفی، چنین حدی وجود ندارد.
در ادامه نشان خواهیم داد که تابع هدف در خوشه‌بندی قاطع با استفاده از مدل آمیخته vMF 
با پارامترهای تراکم برابر و نیز وزن‌های برابر برای اجزا
(که با نام
{\mmm{توزیع آمیخته vMF کروی}{spherical mixture of vMF distributions}}
شناخته می‌شود%
~\cite{banerjee_clustering_2005})
همان تابع هدف در الگوریتم
\lr{k-means}
است.

برای توزیع‌های آمیخته vMF کروی، تابع هدفی که مورد بیشینه‌سازی قرار می‌گیرد به صورت زیر است:








