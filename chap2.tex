\chapter{مدل‌های آمیخته} \label{ch:mixtureModels}
%TODO ref McLachlan book
مدل‌های آمیخته
{\mmm{محدود}{finite mixture models}}
که پس از این به اختصار با عنوان مدل‌های آمیخته به آن‌ها اشاره می‌کنیم، یکی از ابزارهای قدرتمند در آمار و احتمالات جهت مدلسازی گونه‌های مختلفی از پدیده‌های تصادفی هستند.
این مدل‌ها در سال‌های اخیر، به علت  برخورداری از قابلیت انعطاف بالا جهت توصیف توزیع‌های احتمالی پیچیده، توجه پژوهشگران زیادی را در حوزه های مختلف کاربردی و تئوری به خود جلب نموده‌اند.
مدل‌های آمیخته در این کاربردها علاوه بر استفاده مستقیم جهت ارائه مدل توصیفی برای توزیع‌ها، به عنوان مبنا در تکنیک‌های مختلفی مانند
{\mmm{تحلیل خوشه‌ها و طبقه‌بندی پنهان}{cluster and latent class analysis}}،
{\mmm{تحلیل افتراقی}{discriminant analysis}}،
{\mmm{تحلیل تصاویر}{image analysis}} و
{\mmm{تحلیل بقا}{survival analysis}} مورد استفاده قرار می‌گیرند{\cite{mclachlan_finite_2004}}.

با توجه به این که هر توزیع پیوسته‌ای را می‌توان با دقت دلخواه با یک مدل آمیخته محدود از توزیع‌های نرمال با واریانس یا کوواریانس یکسان تخمین زد، مدل‌های آمیخته یک چارچوب
{\mmm{نیمه‌پارامتری}{semiparametric}} مناسب برای مدل کردن شکل‌های ناشناخته توزیع فراهم می‌کنند. مدل‌های آمیخته قابلیت مدل نمودن توزیع‌های کاملا پیچیده را توسط انتخاب مناسب
{\mmm{اجزا}{components}}ی خود جهت بازنمایی دقیق  نواحی محلی
{\mmm{تکیه‌گاه}{support}} توزیع حقیقی دارا هستند.
بنابراین این مدل‌ها می‌توانند در حالاتی که یک خانواده پارامتری واحد قادر به فراهم نمودن یک مدل رضایت‌بخش برای تغییرات محلی در داده‌های مشاهده شده نیست، مفید واقع شوند.

\section{تعاریف اولیه}
تابع چگالی احتمال برای مدل آمیخته به صورت یک جمع وزن داده شده از خانواده‌ای از چگالی‌های پارامتری تعریف می‌شود. به عنوان مثال اگر چگالی احتمال
$f(x|\theta)$
را به عنوان چگالی احتمال اجزای مدل آمیخته در نظر بگیریم که در آن
$\theta$
پارامترهای توزیع را نمایش می‌دهد، چگالی احتمال مدل آمیخته با رابطه زیر مشخص می‌شود:
$$p_X(x)=\sum_{k=1}^{K}w_k f(x|\theta_k).$$

ضرایب
$w_k$
با عنوان
{\mmm{وزن‌های تلفیق}{mixing weights}}
اجزا شناخته می‌شوند.
برای این که
$p_X(x)$
یک تابع چگالی با تعریف درست باشد، وزن‌ها باید مثبت بوده و حاصل جمعشان یک شود:
$$w_k \geq 0,$$
$$\sum_{k=1}^{K}w_k=1.$$

نمونه‌ای از مدل‌های آمیخته که در کاربردهای بسیاری استفاده می‌شود،
{\mmm{مدل آمیخته گوسی}{Gaussian mixture model}}
است که در آن، تابع چگالی $f$ دارای توزیع گوسی با بردار میانگین
$\bm{\mu}$
و ماتریس کوواریانس
$\bm{\Sigma}$
است:
$$p_X(\bm{x})=\sum_{k=1}^{K}w_k (2\pi)^{-\frac{n}{2}}|\bm{\Sigma}|^{-\frac{1}{2}}
 \exp\left(-\frac{1}{2}(\bm{x}-\bm{\mu})^T\bm{\Sigma}^{-1}(\bm{x}-\bm{\mu}) \right).$$

یکی از ویژگی‌های قابل توجه در مدل‌های آمیخته، سادگی تولید نمونه‌های تصادفی از مدل است. برای این منظور کافیست که با احتمال
$w_k$
از جزء $k$ام،
$f(x|\theta_k)$،
نمونه تصادفی تولید شود. مشاهده می‌شود که می‌توان وزن‌های
$w_k$
را به مثابه یک
{\mmm{توزیع پیشین}{prior distribution}}
روی برچسب پنهان جزء مربوطه در نظر گرفت:
$$p_X(x)=\sum_{k=1}^{K}p_K(k)p_X(x|k).$$

\section{مدل آماری استنتاج با توزیع‌های آمیخته}
به منظور انجام استنتاج آماری با استفاده از مدل‌های آمیخته، مسائلی وجود دارد که باید در نظر گرفته شوند.
%TODO figure
اولین نکته‌ای که باید در مدل آماری در نظر گرفته شود این است که مدل باید اطلاعات پیشین ما در مورد داده‌ها را انعکاس دهد که این کار در مدل‌های آمیخته با انتخاب مناسب توزیع‌های احتمال اجزای مدل انجام می‌شود.

تخمین پارامترهای مدل از طریق کمینه یا بیشینه نمودن یک تابع هدف مشخص انجام می‌شود.
یکی از معیارهایی که معمولا در تابع هدف مورد استفاده قرار می‌گیرد و کار تئوری زیادی در خصوص آن انجام شده است، معیار
{\mmm{دیورژانس KL}{KL-divergence}}
است که میزان تطبیق توزیع احتمال مدل را با توزیع حقیقی که داده‌ها از آن گرفته شده‌اند می‌سنجد.
این معیار برآمده از یک درک شهودی در حوزه نظریه اطلاعات است و تعداد بیت اضافی لازم برای کدگذاری داده‌ها در مدل فرض شده را در مقایسه با میزان اطلاعات موجود در داده‌های اصلی مورد اندازه‌گیری قرار می‌دهد.
اگر دو متغیر تصادفی
$\bm{P}$
و
$\bm{Q}$
به ترتیب با توزیع‌های
$p_{\bm{X}}$
و
$q_{\bm{X}}$
داشته باشیم، دیورژانس \lr{KL} بین این دو متغیر تصادفی به صورت زیر تعریف می‌شود:
\begin{eqnarray}\label{eq:kl}
\begin{split}
KL(P \parallel Q) & =\int p_{\bm{X}}(x) \log \frac{p_{\bm{X}}(x)}{q_{\bm{X}}(x)} dx\\
& =\int p_{\bm{X}}(x) \log p_{\bm{X}}(x) dx -
\int p_{\bm{X}}(x) \log q_{\bm{X}}(x) dx.
\end{split}
\end{eqnarray}

در این رابطه قرینه انتگرال اول،
$$H_{\bm{P}}(x) = - \int p_{\bm{X}}(x) \log p_{\bm{X}}(x) dx,$$
آنتروپی توزیع
$p_{\bm{X}}$
است و میزان اطلاعات موجود در آن را اندازه‌گیری می‌کند.
انتگرال دوم،
$$E_{\bm{P}}[-\log q_{\bm{X}}(x)] = \int p_{\bm{X}}(x) \log q_{\bm{X}}(x) dx,$$
{\mmm{آنتروپی متقاطع}{cross entropy}}
بین
$\bm{P}$
و
$\bm{Q}$
است و تعداد بیت‌های مورد نیاز برای کدگذاری داده‌ها در توزیع
$q_{\bm{X}}$
را اندازه گیری می‌کند. دیورژانس \lr{KL} همیشه مقداری نامنفی است و فقط در حالتی که دو متغیر تصادفی
$\bm{P}$
و
$\bm{Q}$
با یکدیگر برابر باشند مقدار صفر خواهد داشت\cite{dembo_information_1991}.

اگر در رابطه \ref{eq:kl} متغیر تصادفی
$\bm{P}$
مربوط به توزیع حقیقی و متغیر تصادفی
$\bm{Q}$
مربوط به توزیع مدل باشد، با جایگزین کردن توزیع مشاهده شده داده‌ها بجای توزیع حقیقی، به سادگی مشاهده می‌شود که
{\mmm{برآورد درست‌نمایی بیشینه}{maximum likelihood estimation}}
هنگامی که اندازه داده‌ها به سمت بی‌نهایت میل کند، مقدار کمینه دیورژانس \lr{KL} را بین توزیع مدل و توزیع حقیقی نتیجه خواهد داد.
%http://www.hongliangjie.com/2012/07/12/maximum-likelihood-as-minimize-kl-divergence/

\section{ملاحظات کاربردی}
الگوریتم بهینه‌سازی استفاده شده برای تخمین پارامترهای مدل، باید ویژگی‌هایی را دارا باشد تا بتوان در عمل از آن استفاده نمود.
یکی از مهمترین این ویژگی‌ها این است که الگوریتم باید از نظر محاسباتی کارآمد باشد یعنی با سرعت قابل قبول به مقادیر بهینه همگرا شود.
در این بخش به بررسی تکنیک‌های مورد نیاز جهت نیل به این مقصود می‌پردازیم.

\subsection{تنظیم در بهینه‌سازی} %TODO footnote "regularization"
یکی از مسائلی که در استفاده از چارچوب درست‌نمایی بیشینه برای تخمین پارامترها باید مورد رسیدگی قرار گیرد، مشکل
{\mmm{تباهیدگی}{degeneracy}}
در تابع لگاریتم درست‌نمایی است به این معنی که ممکن است مقدار لگاریتم درست‌نمایی در بعضی نواحی به سمت بی‌نهایت میل کند.
به عنوان مثال در یک مدل آمیخته گوسی تک‌متغیره:








